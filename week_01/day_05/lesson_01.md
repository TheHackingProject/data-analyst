# Le Big Data
CommenÃ§ons par le commencement : comment obtient-on des donnÃ©es dans la pratique ?

## 1. Introduction
Maintenant que tu connais les langages indispensables Ã  un bon Data Analyst, il faut que tu comprennes le processus gÃ©nÃ©ral d'analyse de donnÃ©es. Ce processus se compose de diffÃ©rentes Ã©tapes. La premiÃ¨re Ã©tape est sans aucun doute l'obtention des donnÃ©es. En effet, pour tout projet, l'entreprise doit en premier lieu identifier la ou les sources de donnÃ©es intÃ©ressantes puis voir comment s'approprier ces sources.

## 2. Historique et contexte
Pendant des siÃ¨cles, lâ€™humanitÃ© a stockÃ© ses donnÃ©es dans les livres ğŸ“. Lâ€™arrivÃ©e du digital au XXÃ¨me siÃ¨cle a donnÃ© une dimension exponentielle et globale Ã  cette pratique. Cette rÃ©volution des donnÃ©es, qui nâ€™est encore quâ€™Ã  ses prÃ©mices, est en train de multiplier les sources de donnÃ©es et de nous donner, Ã  chacun, la possibilitÃ© d'Ã©tudier ces multiples donnÃ©es.

Plus prÃ©cisÃ©ment, environ 2,5 trillions d'octets de donnÃ©es sont produits chaque jour. Ce sont des informations provenant de sources divers : messages, vidÃ©os, informations climatiques, signaux GPS, transactions etc. Sur Facebook seulement, nous envoyons 10 milliards de messages par jour. La quantitÃ© de donnÃ©es digitales produites double tous les 2 ans. En d'autres termes, on a produit autant de donnÃ©es digitales ces 2 derniÃ¨res annÃ©es que tout ce qui a Ã©tÃ© produit auparavant. 

Maintenant que l'on sait que toutes ces donnÃ©es existent, nous allons voir comment on rÃ©cupÃ¨re ces donnÃ©es â­ï¸â­ï¸.

## 3. La ressource

### 3.1. Les sources de donnÃ©es
Une source de donnÃ©es est lâ€™endroit dâ€™oÃ¹ proviennent les donnÃ©es utilisÃ©es. Elle peut Ãªtre lâ€™endroit oÃ¹ les donnÃ©es ont Ã©tÃ© crÃ©Ã©es ou celui oÃ¹ les informations physiques ont Ã©tÃ© numÃ©risÃ©es. 

ConcrÃ¨tement, une source de donnÃ©es peut Ãªtre : 
- un fichier plat, 
- une base de donnÃ©es, 
- des mesures provenant directement dâ€™appareils physiques, 
- des donnÃ©es obtenues par web scraping ou 
- lâ€™une des nombreux services de donnÃ©es en streaming qui abondent sur Internet (analyse du parcours de navigation des internautes, jeux en ligne, e-commerce etc.).

Voici un exemple dâ€™une source de donnÃ©es en action : une marque de mode qui vend des produits en ligne. Pour indiquer quâ€™un article est en rupture de stock, le site web collecte des informations dans une base de donnÃ©es dâ€™inventaire. Dans ce cas de figure, les tableaux dâ€™inventaire sont une source de donnÃ©es, Ã  laquelle accÃ¨de lâ€™application web pour afficher le site web aux clients.


### 3.2. Comment accÃ©der Ã  ces sources de donnÃ©es via Python

#### 3.2.1 AccÃ©der Ã  des donnÃ©es dans un fichier 
Comme nous l'avons dÃ©jÃ  vu, la bibliothÃ¨que Pandas de Python permet de rÃ©cupÃ©rer des donnÃ©es depuis quasiment tous les types de fichiers (CSV, JSON, Excel ...). Cf. la [doc](https://pandas.pydata.org/pandas-docs/dev/user_guide/io.html) pour voir tous les types d'inputs qui peuvent Ãªtre traitÃ©s. 

Pour lire un fichier CSV par exemple et obtenir un DataFrame, il faudra faire ceci : 
```
>>> import pandas as pd
>>> filename = 'chemin/vers/fichier.csv'
>>> df = pd.read_csv(filename)
```

#### 3.2.2 AccÃ©der Ã  des donnÃ©es dans une base de donnÃ©es
Lors des journÃ©es de mercredi et jeudi, vous avez interrogÃ© une base de donnÃ©es en dialoguant avec un SGBD. Mais cela ne se passe pas toujours comme Ã§a. 

___

ğŸ’¡ğŸ’¡  LUMIERE ğŸ’¡ğŸ’¡

Dans la plupart des cas auxquels tu es habituÃ©, lâ€™utilisateur final se trouve devant un programme, qui peut Ãªtre par exemple une application standard ou une page web, qui sert dâ€™intermÃ©diaire entre lâ€™utilisateur et le SGBD. Lâ€™utilisateur nâ€™interagit donc pas directement avec le SGBD, mais avec lâ€™application intermÃ©diaire. Selon les interactions avec lâ€™utilisateur, lâ€™application se charge de construire des requÃªtes au SGBD et de renvoyer le rÃ©sultat Ã  lâ€™utilisateur de maniÃ¨re conviviale.

Ã€ titre dâ€™exemple, considÃ©rons une application fictive de recherche dâ€™horaires de train. Voici un scÃ©nario possible de fonctionnement de cette application :
1. Lâ€™application demande Ã  lâ€™utilisateur : Â« Quelle est la ville de dÃ©part ? Â».
2. Lâ€™utilisateur rÃ©pond Â« Caussade Â».
3. Lâ€™application demande Ã  lâ€™utilisateur : Â« Quelle est la ville dâ€™arrivÃ©e ? Â».
4. Lâ€™utilisateur rÃ©pond Â« Brive-la-Gaillarde Â».
5. Lâ€™application envoie au SGBD la requÃªte SQL :
```
SELECT heureDep, heureArr FROM trains
WHERE villeDep = â€™Caussadeâ€™ AND villeArr = â€™Brive-la-Gaillardeâ€™;
```
6. Le SGBD rÃ©pond au programme en envoyant lâ€™ensemble de tuples {(15h47, 17h22); (18h21, 19h58)}
7. Le programme affiche Ã  lâ€™utilisateur :
RÃ©sultats pour le trajet Caussade -> Brive-la-Gaillarde
Premier train : 15h47 -> 17h22
Second train : 18h21 -> 19h58

___

On peut schÃ©matiser le fonctionnement ainsi :
![fonctionnement](https://github.com/TheHackingProject/data-analyst/blob/master/week_01/day_05/Capture%20d%E2%80%99e%CC%81cran%202021-08-23%20a%CC%80%2017.56.06.png)


Mais s'il n'y a pas d'application dÃ©jÃ  codÃ©e comme lorsque tu rÃ©serves tes billets de train ğŸ˜¤, câ€™est un programme Python qui peut jouer le rÃ´le dâ€™application intermÃ©diaire. Le dialogue se fera avec un SGBD, comme le SGBD SQLite3 par exemple. La plupart des langages de programmation ont des bibliothÃ¨ques permettant Ã  un programme de dialoguer avec la plupart des SGBD existants. Le couple Python-SQLite3 ne dÃ©roge pas Ã  la rÃ¨gle, et câ€™est le module 'sqlite3' qu'il faut utiliser.

Pour interagir avec la base de donnÃ©es via Python, il y a plusieurs Ã©tapes et Ã§a peut Ãªtre fastidieux. 
Retrouve ces diffÃ©rentes Ã©tapes dans [ce cours](https://python.antoinepernot.fr/cours.php?course=chap6). 

Retiens les 3 Ã©tapes principales : 
1) Installer puis importer le connecteur (ex : 'mysql.connector' ou 'sqlite3')
2) Se connecter Ã  la base de donnÃ©es en inscrivant la bonne configuration
3) Extraire des donnÃ©es Ã  partir de la base puis fermer la connexion.


#### 3.2.3 AccÃ©der Ã  des donnÃ©es en scrapant le web

Le web scraping est une technique d'extraction des donnÃ©es de site Internet afin de les enregistrer puis de les analyser. En Python, l'extraction des donnÃ©es depuis le web est souvent rÃ©alisÃ©e Ã  l'aide du module 'BeautifulSoup'.

Pour apprendre Ã  scraper avec Python, je te recommande [cet article](https://medium.com/france-school-of-ai/web-scraping-avec-python-apprenez-%C3%A0-utiliser-beautifulsoup-proxies-et-un-faux-user-agent-d7bfb66b6556) ou cette [vidÃ©o Youtube](https://www.youtube.com/watch?v=Wvc2ZqdIPpk), en fonction du format que tu prÃ©fÃ¨res.


#### 3.2.4 AccÃ©der Ã  des donnÃ©es via une API

MÃªme si le web scraping a son utilitÃ©, ce nâ€™est pas la mÃ©thode Ã  privilÃ©gier pour obtenir des donnÃ©es des sites Internet. En effet, il existe souvent une meilleure mÃ©thode : de nombreux exploitants de sites Internet mettent Ã  disposition les donnÃ©es dans un format structurÃ©, lisible par une machine. Pour accÃ©der aux donnÃ©es, on utilise alors des interfaces de programmation spÃ©ciales, appelÃ©es *Application Programming Interfaces* (API).

Les avantages de lâ€™utilisation dâ€™une API sont significatifs :
- Lâ€™API est explicitement mise Ã  disposition par le fournisseur pour accÃ©der aux donnÃ©es : les risques juridiques sont donc plus faibles et le fournisseur est mieux Ã  mÃªme de rÃ©glementer lâ€™accÃ¨s aux donnÃ©es. Une clÃ© API peut par exemple Ãªtre exigÃ©e pour accÃ©der aux donnÃ©es. Par ailleurs, le fournisseur peut mettre en place une limitation plus prÃ©cise du dÃ©bit.
- Lâ€™API fournit directement les donnÃ©es dans un format lisible par la machine : par consÃ©quent, il nâ€™est pas nÃ©cessaire dâ€™extraire les donnÃ©es du code source, une Ã©tape fastidieuse dans le web scraping. Dâ€™autre part, la structure des donnÃ©es est sÃ©parÃ©e de la reprÃ©sentation visuelle. La structure est ainsi conservÃ©e mÃªme si le design du site Internet est modifiÃ©.

En Python, la librairie la plus utilisÃ©e est Requests. Voici un tuto qui t'explique [comment dÃ©marrer avec la librairie Requests en Python](https://www.digitalocean.com/community/tutorials/how-to-get-started-with-the-requests-library-in-python-fr).


## 4. Points importants Ã  retenir
Le processus d'analyse de donnÃ©es (ou *Data Analysis*) peut Ãªtre dÃ©composÃ© en plusieurs phases. La premiÃ¨re Ã©tape est la collecte de donnÃ©es. Les donnÃ©es sont en provenance dâ€™une ou plusieurs sources. 

Avant de collecter les donnÃ©es, il est primordial de se fixer des objectifs prÃ©cis pour ensuite faire le bon choix dans les sources de donnÃ©es Ã  utiliser, ainsi que dans les mÃ©thodes Ã  employer.


## 5. Pour aller plus loin
Un [article Medium](https://medium.com/@rachidj/collecter-les-donn%C3%A9es-rapidement-et-efficacement-df7dd78b1ac0) d'un data scientist sur la collecte de donnÃ©es. Tu verras comment on peut s'amuser quand on commence Ã  maÃ®triser les techniques vues aujourd'hui.
Et tu peux regarder [cette vidÃ©o](https://www.youtube.com/watch?v=HYNZixyYrW4) si Ã§a t'intÃ©resse de savoir comment scraper un profil Instagram ğŸ‘¹ğŸ‘¹ğŸ‘¹.

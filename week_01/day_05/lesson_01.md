# Le Big Data
A quoi correspond techniquement cette r√©volution qu'on appelle "Big Data" et quelles sont ses cons√©quences sur l'analyse de donn√©es ?

## 1. Introduction
Tu viens de voir les deux familles d'outils indispensables √† la Data Analyse : les langages de programmation et les bases de donn√©es. Il te reste √† voir encore deux familles d'outils : les outils d'analyse (que tu verras lors de la troisi√®me semaine de la formation) et les outils Big Data, qu'on va voir aujourd'hui. Les outils Big Data sont apparus r√©cemment et sont souvent g√©r√©s par des d√©veloppeurs vue leur complexit√©. Mais il est important pour toi de comprendre leur utilit√© et leur fonctionnement.

## 2. Historique et contexte
On peut dater l‚Äôacte de naissance du big data en 2001 avec l‚Äôinvention de la r√®gle des 3V (Volume, Vitesse et Vari√©t√©). A l‚Äô√©poque, l‚Äôexpression traduisait une rupture dans le volume des donn√©es √† traiter. Jusqu‚Äô√† la fin des ann√©es 90, les quantit√©s de donn√©es restaient limit√©es. Puis, on a assist√© √† une explosion du volume de donn√©es avec l‚Äôessor de l‚Äôe-commerce, des r√©seaux sociaux, des terminaux mobiles et, plus r√©cemment, de l‚Äôinternet des objets (IoT). Face √† cette avalanche de data, les mod√®les techniques existants ont montr√© leurs limites. La base de donn√©es parfaite n‚Äôexiste plus. En fonction du souhait de privil√©gier la volum√©trie, la vitesse ou les capacit√©s de requ√™tage, on choisira une solution plut√¥t qu‚Äôune autre ou bien une combinaison d‚Äôoutils.

Pour leurs propres besoins, les GAFAM ont d√ª cr√©er des outils pour stocker et traiter √† la vol√©e des donn√©es √† la fois nombreuses et versatiles, leur structuration changeant avec le temps. Facebook est ainsi √† l‚Äôorigine de Cassandra avant de se tourner vers HBase (NoSQL), Google de BigTable et GFS (anc√™tre d‚ÄôHDFS) et plus r√©cemment de TensorFlow (machine learning). Les g√©ants du web ont ensuite vers√© ces projets en open source, externalisant en quelque sorte leur R&D. Car √† leurs yeux, l‚Äôor ce sont les donn√©es elles-m√™mes, pas les technologies. 
 
Finalement, le buzzword "Big Data" fait avant tout r√©f√©rence √† un ensemble de technologies comme Hadoop, Spark, Kafka ou les bases de donn√©es NoSQL ... que tu vas d√©couvrir aujourd'hui.

## 3. La ressource

https://matheo.uliege.be/bitstream/2268.2/2562/4/M%C3%A9moire%20Camille%20Marenne.pdf

https://inventiv-it.fr/big-data-devez-apprendre/

https://www.atys-concept.com/blog-de-la-performance/articles-performance-industrielle/differences-entre-data-analytics-data-science-big-data/

https://www.decideo.fr/Les-6-competences-les-plus-recherchees-en-Big-Data_a10051.html


### 3.1. Les sources de donn√©es
Une source de donn√©es est l‚Äôendroit d‚Äôo√π proviennent les donn√©es utilis√©es. Elle peut √™tre l‚Äôendroit o√π les donn√©es ont √©t√© cr√©√©es ou celui o√π les informations physiques ont √©t√© num√©ris√©es. 

Concr√®tement, une source de donn√©es peut √™tre : 
- un fichier plat, 
- une base de donn√©es, 
- des mesures provenant directement d‚Äôappareils physiques, 
- des donn√©es obtenues par web scraping ou 
- l‚Äôune des nombreux services de donn√©es en streaming qui abondent sur Internet (analyse du parcours de navigation des internautes, jeux en ligne, e-commerce etc.).

Voici un exemple d‚Äôune source de donn√©es en action : une marque de mode qui vend des produits en ligne. Pour indiquer qu‚Äôun article est en rupture de stock, le site web collecte des informations dans une base de donn√©es d‚Äôinventaire. Dans ce cas de figure, les tableaux d‚Äôinventaire sont une source de donn√©es, √† laquelle acc√®de l‚Äôapplication web pour afficher le site web aux clients.


### 3.2. Diff√©rences entre Big Data et Business Intelligence ?
#### 3.2.1 Acc√©der √† des donn√©es dans un fichier 
Comme nous l'avons d√©j√† vu, la biblioth√®que Pandas de Python permet de r√©cup√©rer des donn√©es depuis quasiment tous les types de fichiers (CSV, JSON, Excel ...). Cf. la [doc](https://pandas.pydata.org/pandas-docs/dev/user_guide/io.html) pour voir tous les types d'inputs qui peuvent √™tre trait√©s. 

Pour lire un fichier CSV par exemple et obtenir un DataFrame, il faudra faire ceci : 
```
>>> import pandas as pd
>>> filename = 'chemin/vers/fichier.csv'
>>> df = pd.read_csv(filename)
```

#### 3.2.2 Acc√©der √† des donn√©es dans une base de donn√©es
Lors des journ√©es de mercredi et jeudi, vous avez interrog√© une base de donn√©es en dialoguant avec un SGBD. Mais cela ne se passe pas toujours comme √ßa. 

___

üí°üí°  LUMIERE üí°üí°

Dans la plupart des cas auxquels tu es habitu√©, l‚Äôutilisateur final se trouve devant un programme, qui peut √™tre par exemple une application standard ou une page web, qui sert d‚Äôinterm√©diaire entre l‚Äôutilisateur et le SGBD. L‚Äôutilisateur n‚Äôinteragit donc pas directement avec le SGBD, mais avec l‚Äôapplication interm√©diaire. Selon les interactions avec l‚Äôutilisateur, l‚Äôapplication se charge de construire des requ√™tes au SGBD et de renvoyer le r√©sultat √† l‚Äôutilisateur de mani√®re conviviale.

√Ä titre d‚Äôexemple, consid√©rons une application fictive de recherche d‚Äôhoraires de train. Voici un sc√©nario possible de fonctionnement de cette application :
1. L‚Äôapplication demande √† l‚Äôutilisateur : ¬´ Quelle est la ville de d√©part ? ¬ª.
2. L‚Äôutilisateur r√©pond ¬´ Caussade ¬ª.
3. L‚Äôapplication demande √† l‚Äôutilisateur : ¬´ Quelle est la ville d‚Äôarriv√©e ? ¬ª.
4. L‚Äôutilisateur r√©pond ¬´ Brive-la-Gaillarde ¬ª.
5. L‚Äôapplication envoie au SGBD la requ√™te SQL :
```
SELECT heureDep, heureArr FROM trains
WHERE villeDep = ‚ÄôCaussade‚Äô AND villeArr = ‚ÄôBrive-la-Gaillarde‚Äô;
```
6. Le SGBD r√©pond au programme en envoyant l‚Äôensemble de tuples {(15h47, 17h22); (18h21, 19h58)}
7. Le programme affiche √† l‚Äôutilisateur :
R√©sultats pour le trajet Caussade -> Brive-la-Gaillarde
Premier train : 15h47 -> 17h22
Second train : 18h21 -> 19h58

___

On peut sch√©matiser le fonctionnement ainsi :
![fonctionnement](https://github.com/TheHackingProject/data-analyst/blob/master/week_01/day_05/Capture%20d%E2%80%99e%CC%81cran%202021-08-23%20a%CC%80%2017.56.06.png)


Mais s'il n'y a pas d'application d√©j√† cod√©e comme lorsque tu r√©serves tes billets de train üò§, c‚Äôest un programme Python qui peut jouer le r√¥le d‚Äôapplication interm√©diaire. Le dialogue se fera avec un SGBD, comme le SGBD SQLite3 par exemple. La plupart des langages de programmation ont des biblioth√®ques permettant √† un programme de dialoguer avec la plupart des SGBD existants. Le couple Python-SQLite3 ne d√©roge pas √† la r√®gle, et c‚Äôest le module 'sqlite3' qu'il faut utiliser.

Pour interagir avec la base de donn√©es via Python, il y a plusieurs √©tapes et √ßa peut √™tre fastidieux. 
Retrouve ces diff√©rentes √©tapes dans [ce cours](https://python.antoinepernot.fr/cours.php?course=chap6). 

Retiens les 3 √©tapes principales : 
1) Installer puis importer le connecteur (ex : 'mysql.connector' ou 'sqlite3')
2) Se connecter √† la base de donn√©es en inscrivant la bonne configuration
3) Extraire des donn√©es √† partir de la base puis fermer la connexion.


#### 3.2.3 Acc√©der √† des donn√©es en scrapant le web

Le web scraping est une technique d'extraction des donn√©es de site Internet afin de les enregistrer puis de les analyser. En Python, l'extraction des donn√©es depuis le web est souvent r√©alis√©e √† l'aide du module 'BeautifulSoup'.

Pour apprendre √† scraper avec Python, je te recommande [cet article](https://medium.com/france-school-of-ai/web-scraping-avec-python-apprenez-%C3%A0-utiliser-beautifulsoup-proxies-et-un-faux-user-agent-d7bfb66b6556) ou cette [vid√©o Youtube](https://www.youtube.com/watch?v=Wvc2ZqdIPpk), en fonction du format que tu pr√©f√®res.


#### 3.2.4 Acc√©der √† des donn√©es via une API

M√™me si le web scraping a son utilit√©, ce n‚Äôest pas la m√©thode √† privil√©gier pour obtenir des donn√©es des sites Internet. En effet, il existe souvent une meilleure m√©thode : de nombreux exploitants de sites Internet mettent √† disposition les donn√©es dans un format structur√©, lisible par une machine. Pour acc√©der aux donn√©es, on utilise alors des interfaces de programmation sp√©ciales, appel√©es *Application Programming Interfaces* (API).

Les avantages de l‚Äôutilisation d‚Äôune API sont significatifs :
- L‚ÄôAPI est explicitement mise √† disposition par le fournisseur pour acc√©der aux donn√©es : les risques juridiques sont donc plus faibles et le fournisseur est mieux √† m√™me de r√©glementer l‚Äôacc√®s aux donn√©es. Une cl√© API peut par exemple √™tre exig√©e pour acc√©der aux donn√©es. Par ailleurs, le fournisseur peut mettre en place une limitation plus pr√©cise du d√©bit.
- L‚ÄôAPI fournit directement les donn√©es dans un format lisible par la machine : par cons√©quent, il n‚Äôest pas n√©cessaire d‚Äôextraire les donn√©es du code source, une √©tape fastidieuse dans le web scraping. D‚Äôautre part, la structure des donn√©es est s√©par√©e de la repr√©sentation visuelle. La structure est ainsi conserv√©e m√™me si le design du site Internet est modifi√©.

En Python, la librairie la plus utilis√©e est Requests. Voici un tuto qui t'explique [comment d√©marrer avec la librairie Requests en Python](https://www.digitalocean.com/community/tutorials/how-to-get-started-with-the-requests-library-in-python-fr).


## 4. Points importants √† retenir
Le processus d'analyse de donn√©es (ou *Data Analysis*) peut √™tre d√©compos√© en plusieurs phases. La premi√®re √©tape est la collecte de donn√©es. Les donn√©es sont en provenance d‚Äôune ou plusieurs sources. 

Avant de collecter les donn√©es, il est primordial de se fixer des objectifs pr√©cis pour ensuite faire le bon choix dans les sources de donn√©es √† utiliser, ainsi que dans les m√©thodes √† employer.


## 5. Pour aller plus loin
Un [article Medium](https://medium.com/@rachidj/collecter-les-donn%C3%A9es-rapidement-et-efficacement-df7dd78b1ac0) d'un data scientist sur la collecte de donn√©es. Tu verras comment on peut s'amuser quand on commence √† ma√Ætriser les techniques vues aujourd'hui.
Et tu peux regarder [cette vid√©o](https://www.youtube.com/watch?v=HYNZixyYrW4) si √ßa t'int√©resse de savoir comment scraper un profil Instagram üëπüëπüëπ.

# Le Big Data
A quoi correspond techniquement cette rÃ©volution qu'on appelle "Big Data" et quelles sont ses consÃ©quences sur l'analyse de donnÃ©es ?

## 1. Introduction
Tu viens de voir les deux familles d'outils indispensables Ã  la Data Analyse : les langages de programmation et les bases de donnÃ©es. Il te reste Ã  voir encore deux familles d'outils : les outils d'analyse (que tu verras lors de la troisiÃ¨me semaine de la formation) et les outils Big Data, qu'on va voir aujourd'hui. Les outils Big Data sont apparus rÃ©cemment et sont souvent gÃ©rÃ©s par des dÃ©veloppeurs vue leur complexitÃ©. Mais il est important pour toi de comprendre leur utilitÃ© et leur fonctionnement.

## 2. Historique et contexte
On peut dater lâ€™acte de naissance du big data en 2001 avec lâ€™invention de la rÃ¨gle des 3V (Volume, Vitesse et VariÃ©tÃ©). A lâ€™Ã©poque, lâ€™expression traduisait une rupture dans le volume des donnÃ©es Ã  traiter. Jusquâ€™Ã  la fin des annÃ©es 90, les quantitÃ©s de donnÃ©es restaient limitÃ©es. Puis, on a assistÃ© Ã  une explosion du volume de donnÃ©es avec lâ€™essor de lâ€™e-commerce, des rÃ©seaux sociaux, des terminaux mobiles et, plus rÃ©cemment, de lâ€™internet des objets (IoT). Face Ã  cette avalanche de data, les modÃ¨les techniques existants ont montrÃ© leurs limites. La base de donnÃ©es parfaite nâ€™existe plus. En fonction du souhait de privilÃ©gier la volumÃ©trie, la vitesse ou les capacitÃ©s de requÃªtage, on choisira une solution plutÃ´t quâ€™une autre ou bien une combinaison dâ€™outils.

Pour leurs propres besoins, les GAFAM ont dÃ» crÃ©er des outils pour stocker et traiter Ã  la volÃ©e des donnÃ©es Ã  la fois nombreuses et versatiles, leur structuration changeant avec le temps. Facebook est ainsi Ã  lâ€™origine de Cassandra avant de se tourner vers HBase (NoSQL), Google de BigTable et GFS (ancÃªtre dâ€™HDFS) et plus rÃ©cemment de TensorFlow (machine learning). Les gÃ©ants du web ont ensuite versÃ© ces projets en open source, externalisant en quelque sorte leur R&D. Car Ã  leurs yeux, lâ€™or ce sont les donnÃ©es elles-mÃªmes, pas les technologies. 
 
Finalement, le buzzword "Big Data" fait avant tout rÃ©fÃ©rence Ã  un ensemble de technologies comme Hadoop, Spark, Kafka ou les bases de donnÃ©es NoSQL ... que tu vas dÃ©couvrir aujourd'hui.

## 3. La ressource

https://matheo.uliege.be/bitstream/2268.2/2562/4/M%C3%A9moire%20Camille%20Marenne.pdf

https://inventiv-it.fr/big-data-devez-apprendre/

https://www.atys-concept.com/blog-de-la-performance/articles-performance-industrielle/differences-entre-data-analytics-data-science-big-data/

https://www.decideo.fr/Les-6-competences-les-plus-recherchees-en-Big-Data_a10051.html


### 3.1. Les sources de donnÃ©es
Une source de donnÃ©es est lâ€™endroit dâ€™oÃ¹ proviennent les donnÃ©es utilisÃ©es. Elle peut Ãªtre lâ€™endroit oÃ¹ les donnÃ©es ont Ã©tÃ© crÃ©Ã©es ou celui oÃ¹ les informations physiques ont Ã©tÃ© numÃ©risÃ©es. 

ConcrÃ¨tement, une source de donnÃ©es peut Ãªtre : 
- un fichier plat, 
- une base de donnÃ©es, 
- des mesures provenant directement dâ€™appareils physiques, 
- des donnÃ©es obtenues par web scraping ou 
- lâ€™une des nombreux services de donnÃ©es en streaming qui abondent sur Internet (analyse du parcours de navigation des internautes, jeux en ligne, e-commerce etc.).

Voici un exemple dâ€™une source de donnÃ©es en action : une marque de mode qui vend des produits en ligne. Pour indiquer quâ€™un article est en rupture de stock, le site web collecte des informations dans une base de donnÃ©es dâ€™inventaire. Dans ce cas de figure, les tableaux dâ€™inventaire sont une source de donnÃ©es, Ã  laquelle accÃ¨de lâ€™application web pour afficher le site web aux clients.


### 3.2. DiffÃ©rences entre Big Data et Business Intelligence ?
#### 3.2.1 AccÃ©der Ã  des donnÃ©es dans un fichier 
Comme nous l'avons dÃ©jÃ  vu, la bibliothÃ¨que Pandas de Python permet de rÃ©cupÃ©rer des donnÃ©es depuis quasiment tous les types de fichiers (CSV, JSON, Excel ...). Cf. la [doc](https://pandas.pydata.org/pandas-docs/dev/user_guide/io.html) pour voir tous les types d'inputs qui peuvent Ãªtre traitÃ©s. 

Pour lire un fichier CSV par exemple et obtenir un DataFrame, il faudra faire ceci : 
```
>>> import pandas as pd
>>> filename = 'chemin/vers/fichier.csv'
>>> df = pd.read_csv(filename)
```

#### 3.2.2 AccÃ©der Ã  des donnÃ©es dans une base de donnÃ©es
Lors des journÃ©es de mercredi et jeudi, vous avez interrogÃ© une base de donnÃ©es en dialoguant avec un SGBD. Mais cela ne se passe pas toujours comme Ã§a. 

___

ğŸ’¡ğŸ’¡  LUMIERE ğŸ’¡ğŸ’¡

Dans la plupart des cas auxquels tu es habituÃ©, lâ€™utilisateur final se trouve devant un programme, qui peut Ãªtre par exemple une application standard ou une page web, qui sert dâ€™intermÃ©diaire entre lâ€™utilisateur et le SGBD. Lâ€™utilisateur nâ€™interagit donc pas directement avec le SGBD, mais avec lâ€™application intermÃ©diaire. Selon les interactions avec lâ€™utilisateur, lâ€™application se charge de construire des requÃªtes au SGBD et de renvoyer le rÃ©sultat Ã  lâ€™utilisateur de maniÃ¨re conviviale.

Ã€ titre dâ€™exemple, considÃ©rons une application fictive de recherche dâ€™horaires de train. Voici un scÃ©nario possible de fonctionnement de cette application :
1. Lâ€™application demande Ã  lâ€™utilisateur : Â« Quelle est la ville de dÃ©part ? Â».
2. Lâ€™utilisateur rÃ©pond Â« Caussade Â».
3. Lâ€™application demande Ã  lâ€™utilisateur : Â« Quelle est la ville dâ€™arrivÃ©e ? Â».
4. Lâ€™utilisateur rÃ©pond Â« Brive-la-Gaillarde Â».
5. Lâ€™application envoie au SGBD la requÃªte SQL :
```
SELECT heureDep, heureArr FROM trains
WHERE villeDep = â€™Caussadeâ€™ AND villeArr = â€™Brive-la-Gaillardeâ€™;
```
6. Le SGBD rÃ©pond au programme en envoyant lâ€™ensemble de tuples {(15h47, 17h22); (18h21, 19h58)}
7. Le programme affiche Ã  lâ€™utilisateur :
RÃ©sultats pour le trajet Caussade -> Brive-la-Gaillarde
Premier train : 15h47 -> 17h22
Second train : 18h21 -> 19h58

___

On peut schÃ©matiser le fonctionnement ainsi :
![fonctionnement](https://github.com/TheHackingProject/data-analyst/blob/master/week_01/day_05/Capture%20d%E2%80%99e%CC%81cran%202021-08-23%20a%CC%80%2017.56.06.png)


Mais s'il n'y a pas d'application dÃ©jÃ  codÃ©e comme lorsque tu rÃ©serves tes billets de train ğŸ˜¤, câ€™est un programme Python qui peut jouer le rÃ´le dâ€™application intermÃ©diaire. Le dialogue se fera avec un SGBD, comme le SGBD SQLite3 par exemple. La plupart des langages de programmation ont des bibliothÃ¨ques permettant Ã  un programme de dialoguer avec la plupart des SGBD existants. Le couple Python-SQLite3 ne dÃ©roge pas Ã  la rÃ¨gle, et câ€™est le module 'sqlite3' qu'il faut utiliser.

Pour interagir avec la base de donnÃ©es via Python, il y a plusieurs Ã©tapes et Ã§a peut Ãªtre fastidieux. 
Retrouve ces diffÃ©rentes Ã©tapes dans [ce cours](https://python.antoinepernot.fr/cours.php?course=chap6). 

Retiens les 3 Ã©tapes principales : 
1) Installer puis importer le connecteur (ex : 'mysql.connector' ou 'sqlite3')
2) Se connecter Ã  la base de donnÃ©es en inscrivant la bonne configuration
3) Extraire des donnÃ©es Ã  partir de la base puis fermer la connexion.


#### 3.2.3 AccÃ©der Ã  des donnÃ©es en scrapant le web

Le web scraping est une technique d'extraction des donnÃ©es de site Internet afin de les enregistrer puis de les analyser. En Python, l'extraction des donnÃ©es depuis le web est souvent rÃ©alisÃ©e Ã  l'aide du module 'BeautifulSoup'.

Pour apprendre Ã  scraper avec Python, je te recommande [cet article](https://medium.com/france-school-of-ai/web-scraping-avec-python-apprenez-%C3%A0-utiliser-beautifulsoup-proxies-et-un-faux-user-agent-d7bfb66b6556) ou cette [vidÃ©o Youtube](https://www.youtube.com/watch?v=Wvc2ZqdIPpk), en fonction du format que tu prÃ©fÃ¨res.


#### 3.2.4 AccÃ©der Ã  des donnÃ©es via une API

MÃªme si le web scraping a son utilitÃ©, ce nâ€™est pas la mÃ©thode Ã  privilÃ©gier pour obtenir des donnÃ©es des sites Internet. En effet, il existe souvent une meilleure mÃ©thode : de nombreux exploitants de sites Internet mettent Ã  disposition les donnÃ©es dans un format structurÃ©, lisible par une machine. Pour accÃ©der aux donnÃ©es, on utilise alors des interfaces de programmation spÃ©ciales, appelÃ©es *Application Programming Interfaces* (API).

Les avantages de lâ€™utilisation dâ€™une API sont significatifs :
- Lâ€™API est explicitement mise Ã  disposition par le fournisseur pour accÃ©der aux donnÃ©es : les risques juridiques sont donc plus faibles et le fournisseur est mieux Ã  mÃªme de rÃ©glementer lâ€™accÃ¨s aux donnÃ©es. Une clÃ© API peut par exemple Ãªtre exigÃ©e pour accÃ©der aux donnÃ©es. Par ailleurs, le fournisseur peut mettre en place une limitation plus prÃ©cise du dÃ©bit.
- Lâ€™API fournit directement les donnÃ©es dans un format lisible par la machine : par consÃ©quent, il nâ€™est pas nÃ©cessaire dâ€™extraire les donnÃ©es du code source, une Ã©tape fastidieuse dans le web scraping. Dâ€™autre part, la structure des donnÃ©es est sÃ©parÃ©e de la reprÃ©sentation visuelle. La structure est ainsi conservÃ©e mÃªme si le design du site Internet est modifiÃ©.

En Python, la librairie la plus utilisÃ©e est Requests. Voici un tuto qui t'explique [comment dÃ©marrer avec la librairie Requests en Python](https://www.digitalocean.com/community/tutorials/how-to-get-started-with-the-requests-library-in-python-fr).


## 4. Points importants Ã  retenir
Le processus d'analyse de donnÃ©es (ou *Data Analysis*) peut Ãªtre dÃ©composÃ© en plusieurs phases. La premiÃ¨re Ã©tape est la collecte de donnÃ©es. Les donnÃ©es sont en provenance dâ€™une ou plusieurs sources. 

Avant de collecter les donnÃ©es, il est primordial de se fixer des objectifs prÃ©cis pour ensuite faire le bon choix dans les sources de donnÃ©es Ã  utiliser, ainsi que dans les mÃ©thodes Ã  employer.


## 5. Pour aller plus loin
Un [article Medium](https://medium.com/@rachidj/collecter-les-donn%C3%A9es-rapidement-et-efficacement-df7dd78b1ac0) d'un data scientist sur la collecte de donnÃ©es. Tu verras comment on peut s'amuser quand on commence Ã  maÃ®triser les techniques vues aujourd'hui.
Et tu peux regarder [cette vidÃ©o](https://www.youtube.com/watch?v=HYNZixyYrW4) si Ã§a t'intÃ©resse de savoir comment scraper un profil Instagram ğŸ‘¹ğŸ‘¹ğŸ‘¹.
